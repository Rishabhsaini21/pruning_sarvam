{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0KPFnx97Z8z"
      },
      "source": [
        "**Pruning Sarvam Model** Kaggle Environment GPU T4\n",
        "\n",
        "Install Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuchEwymB-eh"
      },
      "source": [
        "In this notebook, we will look at an example of depth pruning, which involves removing entire layers from the model.\n",
        "\n",
        "The first thing to note is that removing entire layers from a transformer model usually has a significant impact on the model's performance. This is a much more drastic architectural change compared to the simple removal of neurons from the MLP layers, as seen in the previous example.\n",
        "\n",
        "For this reason, these models are not designed to be used directly after the pruning process. Instead, they will require a subsequent fine-tuning process to recover their capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWf9qpQL7T7L"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers accelerate datasets lm-eval sacrebleu evaluate torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4IFrjlZ7fxq"
      },
      "outputs": [],
      "source": [
        "!pip install -q lm-eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-17q62S-jzX"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHAMiseN7kEi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from lm_eval import evaluator, tasks, models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "import tempfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lphXkDd7nqg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8dC3lat7rb2"
      },
      "source": [
        "Download Model and study Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tsPk2Yo7wE8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"sarvamai/sarvam-1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sarvamai/sarvam-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQXNyc6L7ymi"
      },
      "outputs": [],
      "source": [
        "#Study Model Architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_output(prompt, model=model, tokenizer=tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "        do_sample=False,          # Disable sampling\n",
        "        num_beams=5,              # Use beam search\n",
        "        early_stopping=True,      # Stop when end-of-sequence token is generated\n",
        "        no_repeat_ngram_size=2    # Prevent repetition of 2-grams\n",
        "    )\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HspWIxfw8L7q"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLt00lZR8OqX"
      },
      "outputs": [],
      "source": [
        "original_param_count = count_parameters(model)\n",
        "print(f\"Original model parameters: {original_param_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--ag0T-u8ggR"
      },
      "source": [
        "In a Transformer model like this, layers are organized hierarchically: the initial layers (closer to the input) tend to capture basic language patterns (such as syntactic structure, common word combinations, etc.), while the intermediate and final layers refine these representations, capturing higher-order relationships, global coherence, and subtle semantic nuances.\n",
        "\n",
        "Removing the initial layers directly undermines the foundation upon which more complex representations are built, leading the model to generate meaningless text sequences. Similarly, removing layers based on weight importance metrics (without considering their position or function) can eliminate layers critical for linguistic cohesion or contextual coherence.\n",
        "\n",
        "On the other hand, removing the final layers, while resulting in a loss of some refinement and specialization capabilities, preserves the initial and middle layers that have already learned fundamental language rules and basic word dependencies.\n",
        "\n",
        "However, this is just an empirical and highly simple test. Later, when we evaluate the model's performance using rankings, we will see that it retains a significant portion of its characteristics. Therefore, we are dealing with a model that can deliver very good results after a small fine-tuning process to recover some of the lost capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jYjJhhU81yW"
      },
      "outputs": [],
      "source": [
        "# ELIMINATE LAST LAYERS OF THE MODEL.\n",
        "def prune_last_layers(model, num_layers_to_remove):\n",
        "    \"\"\"\n",
        "    Removes the last 'num_layers_to_remove' layers from the model.\n",
        "\n",
        "    Args:\n",
        "    - model: The model from which layers will be pruned.\n",
        "    - num_layers_to_remove: Number of layers to remove from the top of the stack.\n",
        "\n",
        "    Returns:\n",
        "    - model: The pruned model with fewer layers.\n",
        "    \"\"\"\n",
        "    total_layers = len(model.model.layers)\n",
        "\n",
        "    # Ensure we are not removing more layers than exist\n",
        "    if num_layers_to_remove >= total_layers:\n",
        "        raise ValueError(\"Number of layers to remove is greater or equal to total layers.\")\n",
        "\n",
        "    # Slice the layers to remove the last ones\n",
        "    new_layers = model.model.layers[:total_layers - num_layers_to_remove]\n",
        "    model.model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "    # Update the model configuration\n",
        "    model.config.num_hidden_layers = len(model.model.layers)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-b7OJef9EOh"
      },
      "source": [
        "**Prune Loop**\n",
        "\n",
        "\n",
        "The update_model function iterates through the blocks within the model's Transformer structure. This structure consists of multiple LlamaDecoderLayer blocks, and each of these blocks contains a pair of LlamaSdpaAttention and LlamaMLP components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTnN8iIJ8-I1"
      },
      "outputs": [],
      "source": [
        "def update_model(model, prune_percent):\n",
        "    \"\"\"\n",
        "    Modifies the model by removing entire layers from the end of the stack,\n",
        "    instead of basing it on importance scores. This is a heuristic approach\n",
        "    that tries pruning the top layers to see if it yields better results.\n",
        "\n",
        "    Args:\n",
        "    - model: Model to prune.\n",
        "    - prune_percent: Percentage of layers to prune from the top.\n",
        "\n",
        "    Returns:\n",
        "    - model: New pruned model with fewer layers.\n",
        "    \"\"\"\n",
        "    ### uncomment this if you want to use weight layer selection.\n",
        "    #model = prune_layers(model, prune_percent)\n",
        "\n",
        "    total_layers = len(model.model.layers)\n",
        "    num_layers_to_remove = int(total_layers * prune_percent)\n",
        "\n",
        "    model = prune_last_layers(model, num_layers_to_remove)\n",
        "    #model = prune_first_layers(model, num_layers_to_remove)\n",
        "\n",
        "    # Update the model configuration to reflect the new number of layers\n",
        "    model.config.num_hidden_layers = len(model.model.layers)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-e4pN519Mvy"
      },
      "source": [
        "**Obtain & test the pruned model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fQ8JJpE9HqW"
      },
      "outputs": [],
      "source": [
        "prune_percent = 0.2  # Prune 20% of neurons\n",
        "model = update_model(model, prune_percent)\n",
        "# Recalculate the number of parameters\n",
        "pruned_param_count = count_parameters(model)\n",
        "reduction_in_params = original_param_count - pruned_param_count\n",
        "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
        "\n",
        "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
        "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
        "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdUchido9cM8"
      },
      "source": [
        "**Study Model Structure after pruning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qoj2BHtT9VDv"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRFBWUD49fxA"
      },
      "outputs": [],
      "source": [
        "new_model_name = 'depth20-sarvam-1-2b'\n",
        "# The directory where the model files (config.json, model.safetensors, etc.) will be saved\n",
        "output_dir = './' + new_model_name\n",
        "\n",
        "import os\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Save the pruned model and tokenizer to the local directory\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\" Pruned Sarvam-1 model saved locally to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teIVPn3N9r2W"
      },
      "outputs": [],
      "source": [
        "# Push the model to your Hugging Face repository\n",
        "\n",
        "model.push_to_hub(new_model_name, private=True)\n",
        "tokenizer.push_to_hub(new_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TepRh-9G9yQG"
      },
      "source": [
        "Evaluating the Model\n",
        "\n",
        "LAMBADA – tests the model’s ability to predict the final word of a sentence using long-range context.\n",
        "\n",
        "BoolQ – a yes/no question-answering task.\n",
        "\n",
        "ARC Easy – multiple-choice questions that test basic reasoning.\n",
        "\n",
        "These tasks give a quick snapshot of how pruning affected understanding, reasoning, and language prediction abilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu-pS4Dw9z4y"
      },
      "outputs": [],
      "source": [
        "def evaluate_local_hf_model(output_dir, tasks=['arc_easy', 'boolq', 'lambada'], num_fewshot=0):\n",
        "    \"\"\"\n",
        "    Evaluates a model saved in a local folder using the 'hf' (Hugging Face) backend\n",
        "    of lm-eval, by passing the local path to the 'pretrained' argument.\n",
        "\n",
        "    Args:\n",
        "    - local_model_path: The local directory path (e.g., './depth20-sarvam-1-2b').\n",
        "    - tasks: A list of tasks to evaluate on.\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary of evaluation metrics.\n",
        "    \"\"\"\n",
        "    # The 'hf' model type in lm-eval accepts a local path for the 'pretrained' argument.\n",
        "    # It must be an absolute or relative path to a directory containing the saved model files.\n",
        "    model_args = f\"pretrained={output_dir},device=cuda,dtype=float16\" # Include dtype to match your loading\n",
        "\n",
        "    print(f\"Loading model from local path for evaluation: {output_dir}\")\n",
        "\n",
        "    # Note: Setting limit=None uses the full evaluation set, which can take time.\n",
        "    results = evaluator.simple_evaluate(\n",
        "      model=\"hf\",\n",
        "      model_args=model_args,\n",
        "      tasks=tasks,\n",
        "      num_fewshot=num_fewshot,\n",
        "      limit=None,\n",
        "      bootstrap_iters=10\n",
        "    )\n",
        "\n",
        "    metrics = results.get('results', {})\n",
        "    return metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMIJrjRb-OIZ"
      },
      "outputs": [],
      "source": [
        "# --- Execution ---\n",
        "local_path = './depth20-sarvam-1-2b' # This must match the output_dir above\n",
        "tasks_to_run = ['lambada', 'boolq', 'arc_easy']\n",
        "\n",
        "# You may want to replace these with Indic-specific benchmarks if Sarvam-1 excels there.\n",
        "# You will need to check if lm-eval supports them or if they are in the Sarvam AI Hugging Face dataset collections (e.g., 'sarvamai/boolq-indic').\n",
        "\n",
        "print(f\"\\n--- Starting Local Evaluation for {local_path} ---\")\n",
        "metrics_pruned = evaluate_local_hf_model(local_path, tasks=tasks_to_run)\n",
        "\n",
        "print(\"\\n--- Pruned Sarvam-1 Evaluation Metrics ---\")\n",
        "print(metrics_pruned)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
