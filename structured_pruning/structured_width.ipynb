{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Pruning Sarvam Model** Kaggle Environment GPU T4"
      ],
      "metadata": {
        "id": "Ao2v2Tfn8E28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Libraries"
      ],
      "metadata": {
        "id": "x13phZ4w2ZSV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz1sRJcp1tsY"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers accelerate datasets lm-eval sacrebleu evaluate torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lm-eval"
      ],
      "metadata": {
        "id": "tCcVwNpB5SYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.3"
      ],
      "metadata": {
        "id": "nqO4Gbxr-iC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from lm_eval import evaluator, tasks, models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "x2Vt3Mex2dsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        ""
      ],
      "metadata": {
        "id": "CKNV08yy2rUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Model and Explore Structure"
      ],
      "metadata": {
        "id": "kHYAUi2w2lCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"sarvamai/sarvam-1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sarvamai/sarvam-1\")"
      ],
      "metadata": {
        "id": "HwT1JXWG2tqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An MLP block typically consists of layers that scale the data to larger dimensions and others that return it to its original size.\n",
        "\n",
        "In the MLP block of the model, we find two projection layers: gat_proj and up_proj, both scaling from 2048 to 8192. The purpose of having two layers projecting to the same intermediate size might be related to gating mechanisms. A gating mechanism selectively controls information flow in neural networks by using learned weights to \"gate\" or filter inputs."
      ],
      "metadata": {
        "id": "B3FCbRxx3O0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Study Model Architecture\n",
        "print(model)"
      ],
      "metadata": {
        "id": "ALuOTigq23JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output(prompt, model=model, tokenizer=tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "        do_sample=False,          # Disable sampling\n",
        "        num_beams=5,              # Use beam search\n",
        "        early_stopping=True,      # Stop when end-of-sequence token is generated\n",
        "        no_repeat_ngram_size=2    # Prevent repetition of 2-grams\n",
        "    )\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated"
      ],
      "metadata": {
        "id": "o14-slmP3D-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n"
      ],
      "metadata": {
        "id": "8F6p02ou3TfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_param_count = count_parameters(model)\n",
        "print(f\"Original model parameters: {original_param_count}\")"
      ],
      "metadata": {
        "id": "r_j8Z-Tr3aS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Maximum Absolute Weight method works because it directly identifies the most influential neurons based on the magnitude of their connections. These neurons are likely responsible for key decisions, making the model more accurate after pruning. The Variance of Weights method, while useful in some contexts, can retain neurons that may not contribute significantly to the task, leading to less coherent model outputs."
      ],
      "metadata": {
        "id": "PGpwEyvm3st-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Maximum Absolute Weight:\n",
        "#The maximum absolute weight in a neuron might indicate its significance.\n",
        "\n",
        "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
        "  \"\"\"\n",
        "  compute neuron pair importance scores (Maximum Absolute Weight)\n",
        "\n",
        "  Args:\n",
        "  - gate_weight: Weight matrix from the gate_proj layer.\n",
        "  - up_weight: Weight matrix from the up_weight layer.\n",
        "\n",
        "  Returns:\n",
        "  - importance_scores: Importance scores for each neuron pair.\n",
        "  \"\"\"\n",
        "\n",
        "  gate_max_abs = torch.max(gate_weight, dim=1).values + torch.abs(torch.min(gate_weight, dim=1).values)\n",
        "  up_max_abs = torch.max(up_weight, dim=1).values + torch.abs(torch.min(up_weight, dim=1).values)\n",
        "  importance_scores = gate_max_abs + up_max_abs\n",
        "  return importance_scores\n"
      ],
      "metadata": {
        "id": "SA6j8Q6F3bu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prunes a specific percentatge of neurons from the MLP (feed forward layers).\n",
        "def prune_neuron_pairs(mlp, prune_percent):\n",
        "    \"\"\"\n",
        "    Reduces the dimensions of the **gate_proj**,**up_proj**, **down_proj**\n",
        "    layers removing the least important neurons.\n",
        "\n",
        "    Args:\n",
        "    - mlp: Layers to prune.\n",
        "    - prune_percent: Percentage of neurons to prune.\n",
        "\n",
        "    Returns:\n",
        "    - new_gate_proj, new_up_proj, new_down_proj:  New pruned layers.\n",
        "    - k: New intermediate size.\n",
        "\n",
        "    \"\"\"\n",
        "    # Extract the weights from the MLP layers\n",
        "    #  these weights are used to calculate each neuron's\n",
        "    #  importance score in the next step.\n",
        "    gate_weight = mlp.gate_proj.weight.data.float()\n",
        "    up_weight = mlp.up_proj.weight.data.float()\n",
        "\n",
        "    #Compute importance stores. Neurons with higher importance scores\n",
        "    # are considered more important and less likely to be pruned.\n",
        "    importance_scores = compute_neuron_pair_importance(gate_weight, up_weight)\n",
        "\n",
        "    #Store the original number of neurons in the intermediate layer.\n",
        "    original_intermediate_size = gate_weight.size(0)\n",
        "    #Computes the number of neurons to prune.\n",
        "    num_neuron_pairs_to_prune = min(int(prune_percent * original_intermediate_size), original_intermediate_size - 1)\n",
        "    #Calculate the number of neurons to keep. The new intermediate size.\n",
        "    k = original_intermediate_size - num_neuron_pairs_to_prune\n",
        "\n",
        "    #Just check that there is no big error calculating k. We can't prune all the neurons.\n",
        "    if k <= 0:\n",
        "        raise ValueError(f\"Invalid number of neuron pairs to keep: {k}. Adjust the prune_percent.\")\n",
        "\n",
        "    #Select the neuros to keep, by obtaining the indices to keep.\n",
        "    _, indices_to_keep = torch.topk(importance_scores, k, largest=True, sorted=True)\n",
        "    indices_to_keep = indices_to_keep.sort().values\n",
        "\n",
        "    #create the new layers\n",
        "    new_gate_proj = nn.Linear(mlp.gate_proj.in_features, k, bias=False).to(device)\n",
        "    new_up_proj = nn.Linear(mlp.up_proj.in_features, k, bias=False).to(device)\n",
        "    new_down_proj = nn.Linear(k, mlp.down_proj.out_features, bias=False).to(device)\n",
        "\n",
        "    #copy weights to the new layers.\n",
        "    new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
        "    new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
        "    new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
        "\n",
        "    #return new layers and intermediate size.\n",
        "    return new_gate_proj, new_up_proj, new_down_proj, k\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "qzN0jHpJ38ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neurons are removed in the prune_neurons function based on the values returned by compute_neuron_pair_importance."
      ],
      "metadata": {
        "id": "d1MOWUP94K7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Iterates throught the model layers and applies pruning.\n",
        "def update_model(model, prune_percent):\n",
        "    \"\"\"\n",
        "    It modifies each mlp layer present in model, to retain only the most\n",
        "    important neurons. Creating new smaller versions of each layer pruned.\n",
        "\n",
        "    Args:\n",
        "    - model: Model to prune.\n",
        "    - prune_percent: Percentage of neurons to prune.\n",
        "\n",
        "    Returns:\n",
        "    - model: New pruned model.\n",
        "    \"\"\"\n",
        "    new_intermediate_size = None\n",
        "\n",
        "    #loop for each model layer.\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        #Since each layer is a LlamaDecoderLayer it contains multiple components\n",
        "        # Attention, MLP and Layer norms. We're targetting MLP component\n",
        "        # by accesing layer.mlp.\n",
        "        mlp = layer.mlp\n",
        "\n",
        "        #Call the prune_neiron_pairs with the layers and receiving the pruned.\n",
        "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(mlp, prune_percent)\n",
        "\n",
        "        #Replace the Origiginal Layers with Pruned Layers.\n",
        "        mlp.gate_proj = new_gate_proj\n",
        "        mlp.up_proj = new_up_proj\n",
        "        mlp.down_proj = new_down_proj\n",
        "\n",
        "        #new_intermediate_size only needs to be set once\n",
        "        if new_intermediate_size is None:\n",
        "            new_intermediate_size = new_size\n",
        "\n",
        "    #Update the model config file.\n",
        "    model.config.intermediate_size = new_intermediate_size\n",
        "\n",
        "    return model\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "JQDpydiA4AYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Obtain & test the pruned model.**"
      ],
      "metadata": {
        "id": "uiyaqEgt4W5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prune_percent = 0.2  # Prune 20% of neurons\n",
        "model = update_model(model, prune_percent)"
      ],
      "metadata": {
        "id": "kf8P0D7h4Q32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Recalculate the number of parameters\n",
        "pruned_param_count = count_parameters(model)\n",
        "reduction_in_params = original_param_count - pruned_param_count\n",
        "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
        "\n",
        "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
        "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
        "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")\n"
      ],
      "metadata": {
        "id": "vKhnIHtf4Z5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Study Model Structure after pruning"
      ],
      "metadata": {
        "id": "KKWO5BDy43Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "XYAZ2hRn4esh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model\n",
        "new_model_name = 'pruned20_sarvam_stw'\n",
        "output_dir = './'+new_model_name\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Pruned model saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "9MsXEFxw4l7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the model to your Hugging Face repository\n",
        "\n",
        "model.push_to_hub(new_model_name, private=True)\n",
        "tokenizer.push_to_hub(new_model_name)"
      ],
      "metadata": {
        "id": "gckX9D7i4yuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the Model\n",
        "\n",
        "LAMBADA – tests the model’s ability to predict the final word of a sentence using long-range context.\n",
        "\n",
        "BoolQ – a yes/no question-answering task.\n",
        "\n",
        "ARC Easy – multiple-choice questions that test basic reasoning.\n",
        "\n",
        "These tasks give a quick snapshot of how pruning affected understanding, reasoning, and language prediction abilities."
      ],
      "metadata": {
        "id": "gaFG1GT75DTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loaded_hf_model(model, tokenizer, tasks=['arc_easy', 'boolq', 'lambada'], num_fewshot=0):\n",
        "    \"\"\"\n",
        "    Evaluates a Hugging Face model already loaded into memory by temporarily saving it\n",
        "    to a folder and using lm-eval's standard local model loading (pretrained=...).\n",
        "    \"\"\"\n",
        "    # Create a temporary directory to store the model\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        print(f\"Saving in-memory model temporarily to: {tmpdir}\")\n",
        "        model.save_pretrained(tmpdir)\n",
        "        tokenizer.save_pretrained(tmpdir)\n",
        "\n",
        "        model_args = f\"pretrained={tmpdir},device=cuda,dtype=float16\"\n",
        "\n",
        "        print(f\"Loading model from temp path for evaluation...\")\n",
        "\n",
        "        results = evaluator.simple_evaluate(\n",
        "            model=\"hf\",\n",
        "            model_args=model_args,\n",
        "            tasks=tasks,\n",
        "            num_fewshot=num_fewshot,\n",
        "            limit=None,\n",
        "            bootstrap_iters=10\n",
        "        )\n",
        "\n",
        "    metrics = results.get('results', {})\n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ee1M_LJ5BYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tasks_to_run = ['lambada', 'boolq', 'arc_easy']\n",
        "\n",
        "print(f\"\\n--- Starting Evaluation for In-Memory Model ---\")\n",
        "metrics_pruned = evaluate_loaded_hf_model(pruned_model, tokenizer, tasks=tasks_to_run)\n",
        "print(\"\\n--- Pruned Sarvam-1 Evaluation Metrics ---\")\n",
        "print(metrics_pruned)"
      ],
      "metadata": {
        "id": "j9Ax4xA76WKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}