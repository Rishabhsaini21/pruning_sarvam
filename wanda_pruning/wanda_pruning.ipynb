{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwxNjmah-vbh"
      },
      "source": [
        "**Pruning Sarvam Model** Kaggle Environment GPU T4\n",
        "\n",
        "Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpWbifuo-YOF"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers accelerate datasets lm-eval sacrebleu evaluate torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peKsQYAD-pNo"
      },
      "outputs": [],
      "source": [
        "!pip install -q lm-eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFkpnma8-q1v"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naCO6fIX-xMd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from lm_eval import evaluator, tasks, models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "import tempfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbbV7ca7_Ae8"
      },
      "source": [
        "Download and Study the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXVd8McD_EIg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"sarvamai/sarvam-1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sarvamai/sarvam-1\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQikvepM_FCS"
      },
      "outputs": [],
      "source": [
        "#Study Model Architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_output(prompt, model=model, tokenizer=tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "        do_sample=False,          # Disable sampling\n",
        "        num_beams=5,              # Use beam search\n",
        "        early_stopping=True,      # Stop when end-of-sequence token is generated\n",
        "        no_repeat_ngram_size=2    # Prevent repetition of 2-grams\n",
        "    )\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U78A5oaAmSV"
      },
      "source": [
        "We choose:\n",
        "\n",
        "SPARSITY_RATIO = 0.50 → prune 50% of weights\n",
        "\n",
        "Use Wikitext-2 as calibration data\n",
        "\n",
        "Use 128 samples to capture activations\n",
        "\n",
        "Only prune the Linear layers inside Attention + MLP blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYNgu3RO_OFh"
      },
      "outputs": [],
      "source": [
        "SPARSITY_RATIO = 0.50  # Prune 50% of the weights (unstructured sparsity)\n",
        "CALIBRATION_SAMPLES = 128 # Number of sentences to use for calibration (activation capture)\n",
        "MAX_SEQ_LEN = 128\n",
        "CALIB_DATASET = \"wikitext\"\n",
        "CALIB_CONFIG = \"wikitext-2-raw-v1\"\n",
        "\n",
        "# Target modules for pruning (typically Linear layers in Attention and MLP)\n",
        "# These names are generally consistent across Llama-style models like Sarvam-1 (which uses SwiGLU and Grouped-Query Attention)\n",
        "ATTENTION_LAYERS_TO_PRUNE = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "MLP_LAYERS_TO_PRUNE = [\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "TARGET_MODULES = ATTENTION_LAYERS_TO_PRUNE + MLP_LAYERS_TO_PRUNE\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9caGibvAA3hx"
      },
      "source": [
        "Before we perform WANDA pruning, we need a small amount of real text data to estimate how important each weight is.\n",
        "This is called calibration data, and it allows us to measure the activation statistics of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEOp2wt2_QZM"
      },
      "outputs": [],
      "source": [
        "#Load calibration data\n",
        "print(f\"Loading calibration dataset: {CALIB_DATASET}/{CALIB_CONFIG}...\")\n",
        "calib_dataset = load_dataset(CALIB_DATASET, CALIB_CONFIG, split=\"train\")\n",
        "\n",
        "\n",
        "def get_calibration_data(data, n_samples):\n",
        "    \"\"\"Tokenizes and batches the first n_samples for activation capture.\"\"\"\n",
        "    tokenized_data = []\n",
        "\n",
        "    # Filter and tokenize a subset of the dataset\n",
        "    for example in data:\n",
        "        text = example.get('text', '')\n",
        "        if text.strip() != '':\n",
        "            inputs = tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=MAX_SEQ_LEN,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\"\n",
        "            )\n",
        "            tokenized_data.append(inputs['input_ids'][0])\n",
        "            if len(tokenized_data) >= n_samples:\n",
        "                break\n",
        "\n",
        "    # Stack the samples into a single tensor\n",
        "    return torch.stack(tokenized_data, dim=0).to(DEVICE)\n",
        "\n",
        "# Prepare the data tensor\n",
        "calib_input_ids = get_calibration_data(calib_dataset, CALIBRATION_SAMPLES)\n",
        "print(f\"Calibration data shape: {calib_input_ids.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjZJi8IaA_5F"
      },
      "source": [
        "This section performs the core of the WANDA pruning algorithm, which removes a percentage of weights (e.g., 50%) from the model without retraining, while preserving the most important parameters.\n",
        "\n",
        "WANDA computes a per-weight importance score using both:\n",
        "\n",
        "Weight magnitude (|W|)\n",
        "\n",
        "Activation strength of its input (‖X‖₂)\n",
        "\n",
        "This lets pruning be input-aware and significantly more accurate than magnitude-only pruning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voDGQ8qe_Xi4"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nLoading model: {MODEL_ID}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16, # Sarvam-1 uses bfloat16\n",
        ").to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# We store the L2 norm of the input activation (X) for each layer's Linear weight (W)\n",
        "act_scales = {}\n",
        "\n",
        "def save_input_hook(name):\n",
        "    def hook(module, input_tensor, output_tensor):\n",
        "        # input_tensor is a tuple/list (input, attention_mask, etc.)\n",
        "        # The first element is the hidden state (activation)\n",
        "        act = input_tensor[0].detach() # shape (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # Reshape to (Batch*Seq, HiddenSize)\n",
        "        act = act.view(-1, act.shape[-1])\n",
        "\n",
        "        # Calculate the L2 norm for each feature (column)\n",
        "        # shape is (HiddenSize,)\n",
        "        act_l2_norm = torch.norm(act, p=2, dim=0)\n",
        "\n",
        "        # Accumulate the L2 norms\n",
        "        if name not in act_scales:\n",
        "            act_scales[name] = act_l2_norm\n",
        "        else:\n",
        "            # Simple aggregation (e.g., taking the max or summing)\n",
        "            # The original paper uses a single forward pass, but aggregating across batches is common practice.\n",
        "            act_scales[name] = torch.max(act_scales[name], act_l2_norm)\n",
        "\n",
        "    return hook\n",
        "\n",
        "#Register hooks on all target Linear layers (W in Y=XW+B)\n",
        "handles = []\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Linear) and any(target in name for target in TARGET_MODULES):\n",
        "        # We target the layer BEFORE the weights are used, which is the input activation (X)\n",
        "        handle = module.register_forward_hook(save_input_hook(name))\n",
        "        handles.append(handle)\n",
        "\n",
        "print(\"Starting activation capture...\")\n",
        "#Run a single forward pass over the calibration data to capture activations\n",
        "with torch.no_grad():\n",
        "    for i in range(calib_input_ids.shape[0]):\n",
        "        input_ids = calib_input_ids[i].unsqueeze(0)\n",
        "        model(input_ids)\n",
        "\n",
        "#Remove hooks after capture\n",
        "for h in handles:\n",
        "    h.remove()\n",
        "print(\"Activation capture complete.\")\n",
        "\n",
        "#Calculate Wanda scores and prune\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Linear) and any(target in name for target in TARGET_MODULES):\n",
        "\n",
        "        print(f\"Pruning layer: {name}...\")\n",
        "        W = module.weight.data\n",
        "        X_norm = act_scales[name]\n",
        "\n",
        "        # Ensure the activation norm is the correct shape for element-wise multiplication\n",
        "        # W shape: (out_features, in_features)\n",
        "        # X_norm shape: (in_features,)\n",
        "        # We need to broadcast X_norm to match W's columns (in_features)\n",
        "        X_norm_broad = X_norm.unsqueeze(0).expand_as(W)\n",
        "\n",
        "        # Wanda Score: |W| * ||X||_2\n",
        "        # This is a per-output (row of W) comparison of score:\n",
        "        wanda_score = torch.abs(W) * X_norm_broad\n",
        "\n",
        "        # Get the total number of weights to prune in this layer\n",
        "        total_weights = wanda_score.numel()\n",
        "        prune_count = int(total_weights * SPARSITY_RATIO)\n",
        "\n",
        "        # Find the threshold value for the smallest 'prune_count' scores\n",
        "        # We use a flattened tensor to find the global threshold for this layer\n",
        "        threshold = torch.kthvalue(wanda_score.flatten(), prune_count).values\n",
        "\n",
        "        # Create a mask: True where score < threshold, False otherwise\n",
        "        mask = (wanda_score < threshold).to(W.dtype)\n",
        "\n",
        "        # Apply the pruning: zero out the weights that fall below the threshold\n",
        "        module.weight.data = module.weight.data * (1.0 - mask)\n",
        "\n",
        "print(f\"\\nModel pruned to {SPARSITY_RATIO * 100}% sparsity.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRuWBy-m_fsv"
      },
      "source": [
        "Checking Sparsity after applying pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQnpP0oB_el6"
      },
      "outputs": [],
      "source": [
        "def check_sparsity(model, target_modules):\n",
        "    \"\"\"Calculates the overall and layer-wise sparsity of the pruned model.\"\"\"\n",
        "    total_weights = 0\n",
        "    zero_weights = 0\n",
        "\n",
        "    print(\"\\n--- Sparsity Check ---\")\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        # Only check the layers you targeted for pruning\n",
        "        if isinstance(module, nn.Linear) and any(target in name for target in target_modules):\n",
        "            weight_tensor = module.weight.data\n",
        "            layer_total = weight_tensor.numel()\n",
        "            layer_zeros = torch.sum(weight_tensor == 0).item()\n",
        "\n",
        "            total_weights += layer_total\n",
        "            zero_weights += layer_zeros\n",
        "\n",
        "            layer_sparsity = (layer_zeros / layer_total) * 100\n",
        "            print(f\"Layer: {name} | Sparsity: {layer_sparsity:.2f}%\")\n",
        "\n",
        "    overall_sparsity = (zero_weights / total_weights) * 100\n",
        "    print(f\"\\nOverall Pruned Weights Count: {zero_weights} / {total_weights}\")\n",
        "    print(f\"*** Overall Model Sparsity: {overall_sparsity:.2f}% ***\")\n",
        "\n",
        "    # Check if the overall sparsity is close to the 50% target\n",
        "    if abs(overall_sparsity - SPARSITY_RATIO * 100) < 1.0:\n",
        "         print(\"Verification successful: Sparsity is close to the target ratio.\")\n",
        "    else:\n",
        "         print(\"WARNING: Sparsity does not match the target ratio.\")\n",
        "\n",
        "# Re-run this function after your Wanda pruning code\n",
        "check_sparsity(model, TARGET_MODULES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk4DBlCZ_kn-"
      },
      "outputs": [],
      "source": [
        "# --- SAVE THE PRUNED MODEL ---\n",
        "PRUNED_MODEL_DIR = \"sarvam_wanda_pruned\"\n",
        "tokenizer.save_pretrained(PRUNED_MODEL_DIR)\n",
        "model.save_pretrained(PRUNED_MODEL_DIR)\n",
        "\n",
        "print(f\"\\nPruned model and tokenizer saved to: {PRUNED_MODEL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYWN7XAI_tof"
      },
      "outputs": [],
      "source": [
        "# Push the model to your Hugging Face repository\n",
        "\n",
        "model.push_to_hub(new_model_name, private=True)\n",
        "tokenizer.push_to_hub(new_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHdlJGT2_uat"
      },
      "source": [
        "Evaluating the Model\n",
        "\n",
        "LAMBADA – tests the model’s ability to predict the final word of a sentence using long-range context.\n",
        "\n",
        "BoolQ – a yes/no question-answering task.\n",
        "\n",
        "ARC Easy – multiple-choice questions that test basic reasoning.\n",
        "\n",
        "These tasks give a quick snapshot of how pruning affected understanding, reasoning, and language prediction abilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5un2EXJy_v4u"
      },
      "outputs": [],
      "source": [
        "def evaluate_loaded_hf_model(model, tokenizer, tasks=['arc_easy', 'boolq', 'lambada'], num_fewshot=0):\n",
        "    \"\"\"\n",
        "    Evaluates a Hugging Face model already loaded into memory by temporarily saving it\n",
        "    to a folder and using lm-eval's standard local model loading (pretrained=...).\n",
        "    \"\"\"\n",
        "    # Create a temporary directory to store the model\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        print(f\"Saving in-memory model temporarily to: {tmpdir}\")\n",
        "        model.save_pretrained(tmpdir)\n",
        "        tokenizer.save_pretrained(tmpdir)\n",
        "\n",
        "        model_args = f\"pretrained={tmpdir},device=cuda,dtype=float16\"\n",
        "\n",
        "        print(f\"Loading model from temp path for evaluation...\")\n",
        "\n",
        "        results = evaluator.simple_evaluate(\n",
        "            model=\"hf\",\n",
        "            model_args=model_args,\n",
        "            tasks=tasks,\n",
        "            num_fewshot=num_fewshot,\n",
        "            limit=None,\n",
        "            bootstrap_iters=10\n",
        "        )\n",
        "\n",
        "    metrics = results.get('results', {})\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWAubAmK_5Mc"
      },
      "outputs": [],
      "source": [
        "tasks_to_run = ['lambada', 'boolq', 'arc_easy']\n",
        "\n",
        "print(f\"\\n--- Starting Evaluation for In-Memory Model ---\")\n",
        "metrics_pruned = evaluate_loaded_hf_model(model, tokenizer, tasks=tasks_to_run)\n",
        "print(\"\\n--- Pruned Sarvam-1 Evaluation Metrics ---\")\n",
        "print(metrics_pruned)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
